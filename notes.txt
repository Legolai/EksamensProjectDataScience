            General notes    Step by Step


File order:
    - googleImageScraper.py
    - create_data_lists.py

    - dataset.py
    - model.py
    - optimizer.py
    - train.py

    - eval.py
    - detect.py
    - app.py

    - utils.py





googleImageScraper.py
    Class with an init and 2 methods ('find_image_urls' and 'save_images') 
    along with a main method to run the googleImageScraper to scrape for images

    The init starts off a short check of parameters, notably 'if not os.path.exists', a new folder will be created
    Followed by initialization of the webdriver, where the '--headless' argument is added to option to make it headless (not open webbrowser)
    Ending with initialization of a number of variables for the class

    'find_image_urls' starts with initialization of some variables it will use, amongst them 'image_urls' that will be returned
    Followed by 4 lines of code to deal with the consent cookie page
    For the while loop to download the images
        continues to run until 'count = self.number_of_images' or 'missed_count = self.max_missed'
        The following if/else 'if indx_2 > 0:' tries to "click" on an image found by google images through 'self.driver.find_element', 
        with multiple try except, resulting in 'missed_count = missed_count + 1' if continuous failure

        If the above is succesful, a bigger image will be shown at the side, which the try/except after 'if indx_2 > 0:',
        will try to get the 'src_link' of the image (has to be http) and will then add it to the 'image_urls'

        The last try/except in this while loop will slightly scroll the page down
    Before ending with 'self.driver.quit()' and 'return image_urls'

    'save_images' is a method taking in an array of 'image_urls' and a boolean 'keep_filenames'
    It consists of one big for loop, that starts out with a try/except for 'image = requests.get(image_url, timeout=5)'
    If succesful, a try/except in 'with Image.open(io.BytesIO(image.content)) as image_from_web:' will make a filename,
    that either keeps original or makes a new one
    followed by saving the image to an 'image_path' ('image_path = os.path.join(self.image_path, filename)')

    The __main__ simply makes a 'googleImageScraper' class, runs 'find_image_urls' followed by 'save_images' with its parameters


create_data_lists.py
    Is a .py file with 2 methods ('create_data_lists' that runs 'parse_annotation') and 1 main to run 'create_data_lists'

    'create_data_lists' takes 2 parameters ('path_to_data' and 'output_folder') and with
    'with os.scandir(f'{path_to_data}/images') as entries:' appends every annotation as an object (gotten from 'parse_annotation')
    of x image to the list 'train_objects' and the x image to the list 'train_images'
    Followed by saving the 2 lists to the 'output_folder' (and also a 'label_map' from 'utils.py')

    'parse_annotation' takes the parameter 'annotation_path' and 
    returns the object '{'boxes': boxes, 'labels': labels, 'difficulties': difficulties}' each of the variables being lists
    In the for loop, through '.find' different parameters will be found such as x,y min and max
    culminating in 'boxes.append([xmin, ymin, xmax, ymax])', 'labels.append(label_map[label])' and 'difficulties.append(difficult)'




dataset.py
    Is a .py file with 3 classes all doing the same but slightly different, currently only the CustomDataset2 class is used

    CustomDataset2 is initialized with 'data_folder: str, split: str, keep_difficult=False' where split is "basically a command" later
    given to the transform method from utils.py to determine how it should transform the dataset (either into a train or test dataset)
    With the path 'data_folder' the images and objects loaded into 'self.images' and 'self.objects' for later use.
        The class has 3 methods
        '__len__(self):' is a simple 'return len(self.images)'
        'collate_fn(self, batch):' is needed by the 'torch.utils.data.DataLoader' so that there can be multiple objects in 1 pictures
        '__getitem__(self, i):' is needed for when the train.py has to enumerate/go through every image
            it first opens the image, followed by initializating 'image, boxes, labels, difficulties' that are to be returned
            after the initialization, all of those variables along with 'self.split' are thrown into the 'transform' method in utils.py
            and then returned (to the train method train.py)

            The 'transform' returns 'new_image, new_boxes, new_labels, new_difficulties'
            'if split == 'TRAIN':' then it do a number of additional steps to change a number of images to be different/distorted
            Before then resizing to our needed size (300x300) along with converting 'new_image' to a normalized "torch tensor"

    CustomDataset is also used in eval.py because it reads csv while the CustomDataset2 reads json files.
    Besides the changes to allow for this, it functionally is the same as CustomDataset2


model.py            ## Extra notes: maybe some more on convolutions
    Is the .py file with our model class along with 4 other auxiliary classes
    
    The model class 'SSD300' inherits 'nn.Module' from pytorch and in the '__init__' the 3 variables 
    'self.base', 'self.aux_convs', 'self.pred_convs' are initialized
    Respecitively they call the 3 classes 'VGGBase', 'AuxiliaryConvolutions' and 'PredictionConvolutions'
        These 3 classes consists of the 'VGGBase' that provides the foundation/basis convolutions from an existing network (VGG-16)
        'AuxiliaryConvolutions' provides a higher level convolution/feature map on top of the base
        'PredictionConvolutions' makes prediction for boundary boxes around objects based on the above 2 convolutions

        Starting with 'VGGBase', we initialize numerous 'nn.Conv2d' according to the **VGG-16 architecture** with some small changes
        Before then calling 'self.load_pretrained_layers()' which is a method in the 'VGGBase' class
        The 'load_pretrained_layers' loads the pretrained VGG16 model from 'torchvision.models.vgg16', afterwards
        some changes are made to fc6, fc7 because of the changes made to conv6 and conv7 earlier
        'forward' for forward propagation uses 'leaky_relu' instead of 'relu'

        'AuxiliaryConvolutions' can be said to be an extension of our 'VGGBase', as it continues from 'self.conv7 = nn.Conv2d'
        ('conv8_1', 'conv8_2', 'conv9_1' up to 'conv11_2')
        Basically more convolutional layers are stacked on top of the base network 'VGGBase', 
        providing more feature maps, each progressively smaller than the last

        'PredictionConvolutions' creates 2 convolutional layers for each feature map (such as conv4_3, conv7, conv8_2 etc.)
        One layer is the localization prediction convolutional layer with 4 filters for each prior at current location
            the 4 filters calculates the offset to the bounding box
        While the other is the class prediction layer with n_classes filters for each prior at current location
            the n_classes calculates a set of n_scores for that prior (0.9 pigeon, 0.1 background as an example likely means pigeon in prior)
        Prior boxes per position in each feature map is "planned ahead of time" (such as 'conv4_3': 4, 'conv7': 6 etc.)

    After the above, the '__init__' of 'SSD300' class ends with 'self.priors_cxcy = self.create_prior_boxes()' to create priors
    
    'create_prior_boxes' method creates the priors in center-size coordinates as defined for our selected feature maps (in correct order).
    To that end there's variables like 'obj_scales' and 'aspect_ratios'
    when done the variable 'prior_boxes' be returned while holding 8732 priors with 4 filters

    The last method 'detect_objects' of 'SSD300' is not used by our 'train.py' but instead in 'eval.py' and 'detect.py'
    Instead, what used in training is the last class of the file 'MultiBoxLoss'
    For 'detect_objects'


    'MultiBoxLoss' is a loss function for object detection
    This is a combination of:
    (1) a localization loss for the predicted locations of the boxes, and
    (2) a confidence loss for the predicted class scores.

    Done by matching the 8732 predicted priors to the ground truth by using Jaccard overlaps (less 0.5 = negative, greather than 0.5 = positive)
    Then the total multibox loss is the aggregate of the positive and most/hard negative (the majority should be negative, 
        so to prevent model getting good at detecting only the background, we only take most negative)


optimizer.py            
    Demon ADAM is an optimizer we're just using, no idea about insides


train.py
    'main'
    Starts with loading the data with 'CustomDataset2' and then parsing it to 'torch.utils.data.DataLoader' with some extra parameters
    Then setup for epochs and decay rate
    And then the model is initialized
    And finally at the end a for loop for the epochs

    'train'
    Is one epoch's training and is pretty straightforward with the notes in it

    


eval.py
    Simply evaluates the produced model (checkpoint) against some test images (and their annotations) that haven't been used in training

detect.py
    For use of the app.py so that it can use the model to detect

app.py
    All about setting up the use of the webcam and the program to run



utils.py







